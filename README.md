# FineTuning-LLMs-LoRA-QLoRA
Implementation of parameter-efficient fine-tuning pipelines using LoRA/QLoRA with NF4 across models such as Llama-1.1B and Mistral-7B on Alpaca dataset doe Q&A specific tasks.
Building GPT from scratch, implementing key components of transformer architecture like tokenization, attention, causal masking, positional encoding, etc.

Below is a demonstration of the fine-tuned model in the form of an interactive chatbot:
<img width="1919" height="940" alt="image" src="https://github.com/user-attachments/assets/81426936-7499-4a54-a629-615d8f696f1e" />

Aniket Das (AniketDas13)
